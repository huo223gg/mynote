{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class KRWordRank:\n",
    "    \"\"\"Unsupervised Korean Keyword Extractor\n",
    "\n",
    "    Implementation of Kim, H. J., Cho, S., & Kang, P. (2014). KR-WordRank: \n",
    "    An Unsupervised Korean Word Extraction Method Based on WordRank. \n",
    "    Journal of Korean Institute of Industrial Engineers, 40(1), 18-33.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_count=5, max_length=10, verbose=False):\n",
    "        self.min_count = min_count\n",
    "        self.max_length = max_length\n",
    "        self.verbose = verbose\n",
    "        self.sum_weight = 1\n",
    "        self.vocabulary = {}\n",
    "        self.index2vocab = []\n",
    "\n",
    "    def scan_vocabs(self, docs):\n",
    "        self.vocabulary = {}\n",
    "        if self.verbose:\n",
    "            print('scan vocabs ... ')\n",
    "        \n",
    "        counter = {}        \n",
    "        for doc in docs:\n",
    "            \n",
    "            for token in doc.split():\n",
    "                len_token = len(token)\n",
    "                counter[(token, 'L')] = counter.get((token, 'L'), 0) + 1\n",
    "                \n",
    "                for e in range(1, min(len(token), self.max_length)):\n",
    "                    if (len_token - e) > self.max_length:\n",
    "                        continue\n",
    "                        \n",
    "                    l_sub = (token[:e], 'L')\n",
    "                    r_sub = (token[e:], 'R')\n",
    "                    counter[l_sub] = counter.get(l_sub, 0) + 1\n",
    "                    counter[r_sub] = counter.get(r_sub, 0) + 1\n",
    "        \n",
    "        counter = {token:freq for token, freq in counter.items() if freq >= self.min_count}\n",
    "        for token, _ in sorted(counter.items(), key=lambda x:x[1], reverse=True):\n",
    "            self.vocabulary[token] = len(self.vocabulary)\n",
    "            \n",
    "        self._build_index2vocab()\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('num vocabs = %d' % len(counter))        \n",
    "        return counter\n",
    "    \n",
    "    def _build_index2vocab(self):\n",
    "        self.index2vocab = [vocab for vocab, index in sorted(self.vocabulary.items(), key=lambda x:x[1])]\n",
    "        self.sum_weight = len(self.index2vocab)\n",
    "    \n",
    "    def extract(self, docs, beta=0.85, max_iter=10, vocabulary=None, bias=None, rset=None):\n",
    "        rank, graph = self.train(docs, beta, max_iter, vocabulary, bias)\n",
    "        \n",
    "        lset = {self.int2token(idx)[0]:r for idx, r in rank.items() if self.int2token(idx)[1] == 'L'}\n",
    "        if not rset:\n",
    "            rset = {self.int2token(idx)[0]:r for idx, r in rank.items() if self.int2token(idx)[1] == 'R'}\n",
    "        \n",
    "        keywords = self._select_keywords(lset, rset)\n",
    "        keywords = self._filter_compounds(keywords)\n",
    "        keywords = self._filter_subtokens(keywords)\n",
    "        \n",
    "        return keywords, rank, graph\n",
    "        \n",
    "    def _select_keywords(self, lset, rset):\n",
    "        keywords = {}\n",
    "        for word, r in sorted(lset.items(), key=lambda x:x[1], reverse=True):\n",
    "            len_word = len(word)\n",
    "            if len_word == 1:\n",
    "                continue\n",
    "\n",
    "            is_compound = False\n",
    "            for e in range(2, len_word):\n",
    "                if (word[:e] in keywords) and (word[:e] in rset):\n",
    "                    is_compound = True\n",
    "                    break\n",
    "\n",
    "            if not is_compound:\n",
    "                keywords[word] = r\n",
    "\n",
    "        return keywords\n",
    "    \n",
    "    def _filter_compounds(self, keywords):\n",
    "        keywords_= {}\n",
    "        for word, r in sorted(keywords.items(), key=lambda x:x[1], reverse=True):\n",
    "            len_word = len(word)\n",
    "\n",
    "            if len_word <= 2:\n",
    "                keywords_[word] = r\n",
    "                continue\n",
    "\n",
    "            if len_word == 3:\n",
    "                if word[:2] in keywords_:\n",
    "                    continue\n",
    "\n",
    "            is_compound = False\n",
    "            for e in range(2, len_word - 1):\n",
    "                if (word[:e] in keywords) and (word[:e] in keywords):\n",
    "                    is_compound = True\n",
    "                    break\n",
    "\n",
    "            if not is_compound:\n",
    "                keywords_[word] = r\n",
    "        \n",
    "        return keywords_\n",
    "\n",
    "    def _filter_subtokens(self, keywords):\n",
    "        subtokens = set()\n",
    "        keywords_ = {}\n",
    "\n",
    "        for word, r in sorted(keywords.items(), key=lambda x:x[1], reverse=True):\n",
    "            subs = {word[:e] for e in range(2, len(word)+1)}\n",
    "            \n",
    "            is_subtoken = False\n",
    "            for sub in subs:\n",
    "                if sub in subtokens:\n",
    "                    is_subtoken = True\n",
    "                    break\n",
    "            \n",
    "            if not is_subtoken:\n",
    "                keywords_[word] = r\n",
    "                subtokens.update(subs)\n",
    "\n",
    "        return keywords_\n",
    "    \n",
    "    def train(self, docs, beta=0.85, max_iter=10, vocabulary=None, bias=None):\n",
    "        if (not vocabulary) and (not self.vocabulary):\n",
    "            self.scan_vocabs(docs)\n",
    "        elif (not vocabulary):\n",
    "            self.vocabulary = vocabulary\n",
    "            self._build_index2vocab()\n",
    "\n",
    "        if not bias:\n",
    "            bias = {}\n",
    "        \n",
    "        graph = self._construct_word_graph(docs)       \n",
    "        \n",
    "        dw = self.sum_weight / len(self.vocabulary)\n",
    "        rank = {node:dw for node in graph.keys()}\n",
    "        \n",
    "        for num_iter in range(1, max_iter + 1):\n",
    "            rank = self._update(rank, graph, bias, dw, beta)\n",
    "            sys.stdout.write('\\riter = %d' % num_iter)\n",
    "        print('\\rdone')\n",
    "        \n",
    "        return rank, graph\n",
    "            \n",
    "    def token2int(self, token):\n",
    "        return self.vocabulary.get(token, -1)\n",
    "\n",
    "    def int2token(self, index):\n",
    "        return self.index2vocab[index] if (0 <= index < len(self.index2vocab)) else None\n",
    "    \n",
    "    def _construct_word_graph(self, docs):\n",
    "        def normalize(graph):\n",
    "            graph_ = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "            for from_, to_dict in graph.items():\n",
    "                sum_ = sum(to_dict.values())\n",
    "                for to_, w in to_dict.items():\n",
    "                    graph_[to_][from_] = w / sum_\n",
    "            return graph_\n",
    "        \n",
    "        graph = defaultdict(lambda: defaultdict(lambda: 0))        \n",
    "        for doc in docs:\n",
    "\n",
    "            tokens = doc.split()\n",
    "\n",
    "            if not tokens:\n",
    "                continue\n",
    "\n",
    "            links = []\n",
    "            for token in tokens:\n",
    "                links += self._intra_link(token)\n",
    "\n",
    "            if len(tokens) > 1:\n",
    "                tokens = [tokens[-1]] + tokens + [tokens[0]]\n",
    "                links += self._inter_link(tokens)\n",
    "\n",
    "            links = self._check_token(links)\n",
    "            if not links:\n",
    "                continue\n",
    "            \n",
    "            links = self._encode_token(links)\n",
    "            for l_node, r_node in links:\n",
    "                graph[l_node][r_node] += 1\n",
    "                graph[r_node][l_node] += 1\n",
    "            \n",
    "        graph = normalize(graph)        \n",
    "        return graph\n",
    "    \n",
    "    def _intra_link(self, token):\n",
    "        links = []\n",
    "        len_token = len(token)\n",
    "        for e in range(1, min(len_token, 10)):\n",
    "            if (len_token - e) > self.max_length:\n",
    "                continue\n",
    "            links.append( ((token[:e], 'L'), (token[e:], 'R')) )            \n",
    "        return links\n",
    "    \n",
    "    def _inter_link(self, tokens):\n",
    "        def rsub_to_token(t_left, t_curr):\n",
    "            return [((t_left[-b:], 'R'), (t_curr, 'L')) for b in range(1, min(10, len(t_left)))]\n",
    "        def token_to_lsub(t_curr, t_rigt):\n",
    "            return [((t_curr, 'L'), (t_rigt[:e], 'L')) for e in range(1, min(10, len(t_rigt)))]\n",
    "\n",
    "        links = []\n",
    "        for i in range(1, len(tokens)-1):\n",
    "            links += rsub_to_token(tokens[i-1], tokens[i])\n",
    "            links += token_to_lsub(tokens[i], tokens[i+1])\n",
    "        return links\n",
    "    \n",
    "    def _check_token(self, token_list):\n",
    "        return [(token[0], token[1]) for token in token_list if (token[0] in self.vocabulary and token[1] in self.vocabulary)]\n",
    "    \n",
    "    def _encode_token(self, token_list):\n",
    "        return [(self.vocabulary[token[0]],self.vocabulary[token[1]]) for token in token_list]\n",
    "    \n",
    "    def _update(self, rank, graph, bias, dw, beta):\n",
    "        rank_new = {}\n",
    "        for to_node, from_dict in graph.items():\n",
    "            rank_new[to_node] = sum([w * rank[from_node] for from_node, w in from_dict.items()])\n",
    "            rank_new[to_node] = beta * rank_new[to_node] + (1 - beta) * bias.get(to_node, dw)\n",
    "        return rank_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "krwordrank = KRWordRank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krwordrank.verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [lovit]",
   "language": "python",
   "name": "Python [lovit]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
